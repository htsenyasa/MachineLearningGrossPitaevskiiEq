\documentclass[a4paper,times,12pt]{article}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{setspace}
\usepackage{svg}
\usepackage{subcaption}
%\usepackage{minted}
\onehalfspacing
\usepackage[top=2.5 cm, bottom=2.5 cm, left=4 cm, right=2.5 cm]{geometry}
\usepackage{hyperref}

\begin{document}

\begin{titlepage}
\begin{center}
\vspace*{1cm}
\underline{\textbf{\Large ISTANBUL TECHNICAL UNIVERSITY}} \\[10 pt]

\underline{\textbf{\large FACULTY OF SCIENCE AND LETTERS}} \\[15 pt]

\textbf{\large Advanced Physics Project Report} \\
\vspace{1.8 cm}
\includegraphics[scale=1.2]{itu_logo.pdf} \\
\vspace{1.8 cm}
%\textbf{\large Machine Learning and Nonlinear Schr{\"o}dinger Equation} \\[5 pt]
\textbf{\large Predicting energy expectation values for one-dimensional non-linear Schr{\"o}dinger Equation in random harmonic potentials using artificial neural network} \\[5 pt]
\textbf{H{\"u}seyin Talha \c{S}enya\c{s}a}\\
\vspace{1.5 cm}
\end{center}
\vfill
\noindent\textbf{{Department : Physics Engineering}}\\
    \textbf{Student ID \hspace{0.2 cm}: 090120132}\\
    \textbf{Advisor\hspace{1.1 cm}: Assoc. Prof. A. Levent Suba\c{s}{\i}}
\vspace{2 cm}

\center\textbf{FALL 2017}

\end{titlepage}


%\title{\textbf{TEST}}
%\date{}
%\maketitle
\setcounter{page}{1}
\pagenumbering{roman}

\section*{Summary}

We train an artificial neural network to estimate the ground state energy
of a one-dimensional Bose-Einstein condensate in harmonic trapping potential.
Such a system can be described by the solution of a non-linear Schr{\"o}dinger equation also called a Gross-Pitaevskii equation. We also use the method for the inverse problem of predicting the non-linearity parameter using the ground
state density profile for a given harmonic trapping potential.

\newpage
\tableofcontents

\newpage

\pagenumbering{arabic}
\section{Introduction and Motivation}
\label{sec:Intro}

Machine learning is a growing research area, not only its theoretical base but also its share in applications in different areas. One of the reason why machine learning is used increasingly today is that artificial neural networks used in machine learning can approximate any continuous function within desired accuracy. This means that if we take $\epsilon > 0$ as desired accuracy,  $\boldsymbol{y} = \boldsymbol{f}(\boldsymbol{x})$ output of the network and $\boldsymbol{g}(\boldsymbol{x})$ real value of the function, then it is guaranteed that there exists a network that satisfies the relation $|\boldsymbol{g}(\boldsymbol{x}) - \boldsymbol{f}(\boldsymbol{x})| < \epsilon $. From this point of view, a process or calculation that can be thought of as a function can be represented by a corresponding neural network that mimics this function in desired accuracy \cite{nielsen2015neural}. With this in mind, neural networks have the potential to learn general functions and be exploited for their advantages. Approximate value of the quantity can be determined for different scenarios.

Many different kind of applications of machine learning have already been implemented in physics\footnote{For a detailed list, see \cite{physicsml}}. For example, In \cite{carleo2017solving, cai2017approximating} machine learning is applied to quantum many body problems. A machine learning method called Unsupervised Learning to detect patterns in big datasets is used for discovering phase transitions \cite{wang2016discovering}. There are also developed techniques in machine learning inspired from physics such as quantum tensor networks \cite{stoudenmire2016supervised}. Relation between physics and machine learning also caused foundation of a new branch called Quantum Machine Learning which aims to implement a quantum software to make machine learning faster than its classical version \cite{biamonte1611quantum}. 

In \cite{mills2017deep}, machine learning approach is applied to a 2D Schr{\"o}dinger Equation with random potentials. The authors built a convolutional deep neural network, and trained it to predict ground state energy of the system under four different two dimensional confining potentials including some random potentials. Their study showed that machine learning is a promising alternative in electronic structure calculations of quantum systems. In our study inspired from the article mentioned above, we apply machine learning method to the nonlinear Schr{\"o}dinger equation to predict ground state energy of a Bose-Einstein condensate at absolute zero temperature in one dimensional harmonic trapping potentials.


\section{Gross Pitaevskii Equation}

A Bose-Einstein Condensate (BEC) is described by Gross Pitaevskii Equation (GPE) also known as non-linear Schrodinger Equation (NLSE). General form of GPE in three dimension is given as,

\begin{equation}
\label{eq:GPE_3D}
i \hbar \frac {\partial \Psi}{\partial t} = \frac {-\hbar^2}{2m}\nabla^2
\Psi + V(\boldsymbol{r}, t)\Psi + g|\Psi|^2\ \Psi
\end{equation}

\noindent where $\hbar$ is Planck constant, $\boldsymbol{r}$ is position vector, $t$ is time, $\Psi(\boldsymbol{r}, t)$ is the wave function, $m$ is mass, $\nabla^2$ is the Laplacian operator, $V$ is the potential, $g$ is a constant interaction parameter and it is defined as 

\begin{equation}
\label{eq:GPE_inter_param}
g=\frac{4\pi\hbar^2a_s}{m}
\end{equation}

\noindent where, $a_s$ is the s wave scattering length. Non-linearity of GPE comes from the interaction term. If there is no interaction GPE reduces to the Schr{\"o}dinger Equation (SE) and becomes a linear equation. By definition, $g$ can be positive or negative. If $g > 0$, it represents repulsive interaction, and if $g < 0$, it represents attractive interactions\cite{barenghi2016primer}.

In Eq.~\eqref{eq:GPE_3D}, the first term on the right side is the kinetic term, the second term represents the potential energy, and the last term is the interaction term of the Hamiltonian. The expectation value of the total energy can be written as;

\begin{equation}
\label{GPE_total_energy_general}
\langle E \rangle = \int \Psi^{*}\hat{H}\Psi d^3\boldsymbol{r}
\end{equation}

\begin{equation}
\label{GPE_total_energy}
E = \int \left(\frac {\hbar^2}{2m}|\nabla
\Psi|^2 + V|\Psi|^2 + g|\Psi|^4 \right) d^3\boldsymbol{r}
\end{equation}

\noindent If potential is not function of time, then, total energy of the system is conversed.

%In general, potential $V$ is time independent harmonic trapping potential \cite{barenghi2016primer}. In such a case, the potential is only function of position $V(\boldsymbol{r})$ and it can be given as,
%

We assume a time-independent harmonic trapping potential. In such a case, the potential is only function of position $V(\boldsymbol{r})$ and it can be written as
\begin{equation}
\label{eq:GPE_harmonic_potential}
V(x, y, z) = \frac{1}{2}m(\omega_x^2 x^2 + \omega_y^2y^2 + \omega_z^2z^2)
\end{equation}


\noindent where $\omega$ is the angular frequency. Then method of separation of variables can be applied and it can be obtained that there is a solution in stationary form. In this case the factorized wave function is,

\begin{equation}
\label{eq:GPE_time_indep_wave_func}
\Psi(\boldsymbol{r}, t) = \psi(\boldsymbol{r})e^{-i\mu t/\hbar}
\end{equation}

\noindent Here, $\mu$ is the chemical potential. Time independent GPE becomes,

\begin{equation}
\label{eq:GPE_time_indep}
\mu\psi = \frac{-\hbar^2}{2m}\nabla^2\psi + V(\boldsymbol{r})\psi + g|\psi|^2\psi 
\end{equation}

BECs can also be studied in one dimension by making angular frequencies asymmetric such that $\omega_x, \omega_y \gg \omega_z$ and keeping their energy order much greater than condensate's energy $ \hbar(\omega_x \omega_y)^{1/2} \gg \mu $. This enables us to confine the dynamics of the system in one dimension and it can be described with a corresponding one dimensional GPE equation \cite{barenghi2016primer}. To obtain one dimensional GPE one can rewrite wave function,

\begin{equation}
\label{eq:GPE_time_indep_wave_func_seperated}
\psi(x, y, z, t) = \psi_z(z, t)\psi_x(x)\psi_y(y)
\end{equation}

\noindent $\psi_x$ and $\psi_y$ are the ground state solution of the transverse potential;

\begin{equation}
\begin{split}
\label{eq:GPE_x_y_wave}
& \psi_x(x) = \frac{1}{{(\pi l_x^2)}^{1/4}}e^{-x^2/2l_x^2} \\
& \psi_y(y) = \frac{1}{{(\pi l_y^2)}^{1/4}}e^{-y^2/2l_y^2}
\end{split}
\end{equation}

\noindent where $l_x = \sqrt{\hbar/m\omega_x}$ and $l_y = \sqrt{\hbar/m\omega_x}$ are the harmonic oscillator lengths that related to the density distribution of the condensate. Normalization of the wave function is chosen such that $\int \psi_x(x)dx = \int \psi_y(y)dy = 1$, and $\int \psi_z(z, t) = N$. If we plug in the factorized wave function to Eq.~\eqref{eq:GPE_time_indep}, then, the equation becomes;

\begin{equation}
\label{eq:GPE_1D}
\mu^{\prime}\psi_z = \frac{-\hbar^2}{2m}\frac{d^2\psi_z}{dz^2} + \frac{1}{2}m\omega_z^2 z^2\psi_z + g^{\prime}|\psi_z|^2\psi_z 
\end{equation}

\noindent where $\mu^{\prime}$ and $g^{\prime}$ one dimensional effective chemical potential and interaction strength respectively

\begin{equation}
\label{eq:GPE_1D_chem_inter}
\mu^{\prime} = \mu - \frac{\hbar}{2}(\omega_x - \omega_y), \quad g^{\prime} = \frac{g}{2\pi l_x l_y}
\end{equation}

\noindent From now on, we refer to Eq.~\eqref{eq:GPE_1D} as GPE.

It is convenient to work with dimensionless quantities, to make equation dimensionless, one can define $z = a_0\widetilde{z}$. In this case Eq.~\eqref{eq:GPE_1D} becomes,

\begin{equation}
\label{eq:GPE_dimensionless_length}
\mu\psi = \frac{-\hbar^2}{2ma_0^2}\frac{d^2\psi}{d\widetilde{z}^2} + \frac{1}{2}m\omega^2 a_0^2 \widetilde{z}^2\psi + g|\psi|^2\psi
\end{equation}

\noindent (Here we dropped primes). After that, setting $\hbar\omega = {\hbar^2}/{m a_0^2}$, we find $a_0^2 = \hbar\omega/m$ .Plugging $a_0^2$ in to the Eq.~\eqref{eq:GPE_dimensionless_length}, we get

\begin{equation}
\label{eq:GPE_dimensionless_energy}
\mu\psi = -\frac{1}{2}m\omega^2\frac{d^2\psi}{d\widetilde{z}^2} + \frac{\hbar^2}{2m}\widetilde{z}^2\psi + g|\psi|^2\psi
\end{equation}

\noindent If we divide Eq.~\eqref{eq:GPE_dimensionless_energy} by $\hbar\omega$ and define $\widetilde{\mu} = \mu/\hbar\omega$, we obtain

\begin{equation}
\label{eq:GPE_dimensionless_without_g}
\widetilde{\mu} \psi = -\frac{1}{2\sqrt{a_0}}\frac{d^2\psi}{d\widetilde{z}^2} + \frac{1}{2\sqrt{a_0}}\widetilde{z}^2\psi + \frac{g|\psi|^2\psi}{\hbar\omega}
\end{equation}

\noindent From the normalization condition,

\begin{equation}
\int |\psi|^2dz = \int |\psi|^2 a_0 d\widetilde{z} = N
\end{equation}

\noindent we can define $\widetilde{\psi} = ({\sqrt{a_0}}/{\sqrt{N}})\psi$. The third term on the right in Eq.~\eqref{eq:GPE_dimensionless_without_g} becomes,

\begin{equation}
\label{eq:GPE_dimensionless_g_1}
\frac{g|\psi|^2}{\hbar\omega} = \frac{g a_0}{N\hbar\omega}|\widetilde{\psi}|^2 
\end{equation}

\noindent Finally, defining

\begin{equation}
\label{eq:GPE_dimensionless_g_2}
\frac{g a_0}{N\hbar\omega_z} = \widetilde{g}
\end{equation}

\noindent the dimensionless GPE can be written as,

\begin{equation}
\label{eq:GPE_dimensionless}
\widetilde{\mu} \widetilde{\psi} = -\frac{1}{2}\frac{d^2\widetilde{\psi}}{d\widetilde{z}^2} + \frac{1}{2}\widetilde{z}^2\widetilde{\psi} + \widetilde{g}|\widetilde{\psi}|^2 \widetilde{\psi}
\end{equation}

There is no known analytic solution of GPE for harmonic trapping potential except the case in which interaction parameter is zero. In this case, GPE reduces to the SE and solution is well known with the ground state energy;

\begin{equation}
\label{eq:GPE_no_inter_GSE}
\widetilde{\mu}= \frac{1}{2}
\end{equation}

\noindent we used this case to check our numerical solutions. 


\subsection{Numerical Solution and XMDS Framework}

In the previous section we stated that there is no general analytic solution of GPE. It is known for only few cases such as for a uniform condensate $V = 0$ constant. Most of the time GPE is solved with numerical techniques or approximations. In this study, we used a framework called XMDS \cite{dennis2013xmds2}, implemented specifically to solve differential equation systems with well optimized numerical methods. In this framework the partial differential equations system can be described by a markup language called XML.  When equation system is declared properly, XMDS produces a source code written in C++ that solves the equation with pre-specified numerical method. In our program, XMDS takes angular frequency, shift of equilibrium point, value of interaction parameter externally. It generates trapping potential and other quantities internally with supplied expressions.


Cross-check of the framework has done in two ways. First, since there exists an analytical solution for $g = 0$, we compared the results of the XMDS with analytical ones, and we also used another program here \cite{muruganandam2009fortran}, and compared few results for situations where analytical solution does not exist.  


\section{Problem Statement and Dataset Generation}
\label{sec:Problem statement}

GPE can be solved with stated methods above. These methods must be applied every time when there is a change in parameters of the equation such as applying different trapping potential or changing interaction parameter. For example, when interaction parameter is changed, if there is no analytic solution, numerical method must be reapplied. With machine learning these steps can be encoded in network and information about the system can be obtained instantly with only one time cost which is training process of the network. In\cite{mills2017deep}, authors apply deep learning method to the Schr{\"o}dinger Equation to obtain ground state energy of the system under different potentials. 

We try to built an artificial neural network to predict the ground state energy of a BEC for a given harmonic trapping potential and interaction parameter. We also try the inverse problem which is the prediction of interaction parameter for a given potential and density. 

An artificial neural network is made of layers and these layers contain simple units called neurons. These neurons were insprired by the neurons in the human brain. A neuron takes one or more input and generates an output. To do this, it uses its internal variables which are called weights and biases. In general, the number of weights in a neuron is equal to the size of the input and there is one bias value. There is no restriction on the range of weights and biases except computational, they can even take on complex values \cite{zimmermann2011comparison}. Input output relation of a neuron can be described in the following way; let $\boldsymbol{x} = [x_1, x_2, x_3, ..., x_n]$ be the input of the neuron and $f$ be the function that describes behavior of the neuron. 

\begin{equation}
\label{eq:NU_neuron}
f(\boldsymbol{x}, \boldsymbol{\omega}, b) = \sum\limits_{i = 1}^n \omega_i x_i + b
\end{equation}

\noindent Here, $\omega_i$ are the weights and $b$ is the bias. A layer of a network involves neurons that implement with this function. In our example, $x_n$ inputs represent the potential expression respect to the position or represents density for the inverse problem. The range of the function $f$ is infinite and it can be used directly. However, there are various ways to interpret result of the function $f$. The most primitive version of interpretation is sending the result of $f$ as an argument to unit step function to generate 0 (False) or 1 (True). In this case, 

\begin{equation}
\label{eq:NU_step_function}
a(f) = u_1(f(\boldsymbol{x}, \boldsymbol{\omega}, b))
\end{equation}

\noindent where $a$ is called as the activation function of the neuron and it is a unit step function in this case. However, the usage of the step function introduces discontinuity and does not allow to use differential methods to calculate behavior of the neurons when a small change in weights or biases is applied. Therefore, a more common activation function is sigmoid function denoted by $\sigma$ which is continuous version of the step function and it is defined as,

\begin{equation}
\label{eq:NU_sigma_function}
\sigma(f) = \frac{1}{1 + e^{-f}}
\end{equation}

If a neuron defined in this way such that its output given by Eq.~\eqref{eq:NU_sigma_function}, then it is called a sigmoid neuron. In the former case which is the step function, the neuron is called as perceptron. 

If we denote $j^{th}$ neuron in the $l^{th}$ layer with $f_j^l$, and the weight from $k^{th}$ neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the layer $l^{th}$ with $\omega_{jk}^l$, finally the bias of the $j^{th}$ neuron in the $l^{th}$ layer with ${b_j^l}$, then, the output of the $j^{th}$ neuron in the $l^{th}$ layer will be;

\begin{equation}
\label{eq:NU_neuron_connection}
f_j^{l}(\sigma(f_k^{l-1}), \boldsymbol{\omega}, b) = \sum\limits_{k=1}^{n} \omega_{jk}^{l}\sigma(f_k^{l-1}) + b_j^l
\end{equation}

\noindent and the activation of the output can be written as;

\begin{equation}
\sigma(f_j^{l}(\sigma(f_k^{l-1}), \boldsymbol{\omega}, b)
\end{equation}


\noindent Here we use sigmoid function as activation function but it can be any suitable activation function such as hyperbolic tangent $\tanh(f)$ or Rectified Linear Unit (ReLU).

Connection complexity of the neurons may vary. Output of each neuron in one layer can be input of every neuron in next layer. Such networks are called Fully Connected Networks (FCN) or multilayer perceptrons (MLP)\cite{nielsen2015neural}. If the result of one layer is directly sent to the next layer without any circulation or feedback, then the network is called as Feed Forward Network \cite{nielsen2015neural}.

We are going to change these weights and biases in such a way that the difference between output generated by network and real value of the corresponding quantity will be minimized. To do that, we are going to use a proxy relation between output and real value which is called as Cost Function \cite{nielsen2015neural}. There are different kind of cost functions such as quadratic cost function, cross entropy cost function etc. We are going to use quadratic cost function also called as mean squared error to show the general mechanism and to introduce few notions that directly effects the behavior of the network. 


The quadratic cost function is defined as;

\begin{equation}
\label{eq:NT_Quadratic}
C(\omega, b) = \frac{1}{2n} \sum\limits_{x} || \boldsymbol{y} - a(f_L(\boldsymbol{x})) ||{^2} 
\end{equation}

\noindent where $n$, $\omega$, $b$ are number of examples in the training set, weights and biases, respectively. The subscript $L$ indicates the last layer which is the output, so $f_L(\boldsymbol{x})$ output produced by network and $\boldsymbol{y}$ is real value of the quantity. To minimize $C$, we can take derivative of the function and can find extremum values, but this method is extremely costly because the total number of neurons in the network is enormous \cite{nielsen2015neural}. This problem can be overcame by an iterative algorithm called Gradient Descent. Since $C$ is also a scalar field the algorithm tries to determine a direction which points to the decrement and moves in that direction with small steps. In each iteration, algorithm repeats itself to reach minimum value. 

A small displacement in arbitrary direction which corresponds to a small change in weight or bias or both can be written as;
\begin{equation}
\label{eq:NT_Quadratic_min}
\Delta{C} = \frac{\partial{C}}{\partial{\omega}}\Delta{\omega} + \frac{\partial{C}}{\partial{b}}\Delta{b}
\end{equation}

\noindent This expression gives information about what happens when small displacement is made. In order to the determine direction of decrement we can rewrite this expression in terms of the gradient since it points to direction of maximum rate of increase. 

We can define gradient operator as;

\begin{equation}
\label{eq:gradient}
\boldsymbol{\nabla}{C} = \left(\frac{\partial{C}}{\partial{\omega}},\frac{\partial{C}}{\partial{b}} \right)^T
\end{equation}

\noindent In this case, Eq.~\eqref{eq:NT_Quadratic_min} can be written as;

\begin{equation}
\label{eq:NT_Quadratic_min_gradient_form}
\Delta{C} = \boldsymbol{\nabla}{C} \cdot \Delta \boldsymbol{l}
\end{equation}

\noindent where $\boldsymbol{l} = (\omega, b)^T$. We want to minimize the cost function, thus; in each iteration value of the $C$ must be smaller than the previous one which means that $\Delta{C}$ must be smaller than zero. Therefore, the direction of displacement must be in the opposite direction of the gradient. Rather than calculating the opposite direction for each iteration we can define $\Delta{\boldsymbol{l}}$ in such a way so that it always points to opposite direction of the gradient.

\begin{equation}
\label{eq:NT_learning_rate}
\Delta{\boldsymbol{l}} = -\eta\boldsymbol{\nabla}C
\end{equation}

\noindent In this case, Eq.~\eqref{eq:NT_Quadratic_min_gradient_form} becomes;

\begin{equation}
\label{eq:NT_Displacement}
\Delta{C} = -\eta ||\boldsymbol{\nabla}{C}||^2
\end{equation}

\noindent where $\eta$ is positive definite number and it is called the learning rate. Since it is guaranteed that $||\boldsymbol{\nabla}{C}||^2 \geq 0$, therefore; $\Delta{C} \leq 0$. In this situation, learning rate is the step size in each iteration. From Eq.~\eqref{eq:NT_Displacement} it is intuitive that $\eta$ can not be a large number because in such a case, the algorithm can miss the minimum point. This situation also brings up the subject that gradient descent does not give guarantee to find the minimum point.

Gradient Descent algorithm has an statistical version called Stochastic Gradient Descent to increase the speed of training process. It is assumed that gradient of  $m$ randomly selected examples in the dataset is nearly equal to gradient of the whole dataset and it can be expressed as;

\begin{equation}
\label{eq:NT_Stochastic}
\frac{1}{m} \sum\limits_{k = 1}^{m} \nabla{C_{X_j}} \approx \frac{1}{n}\sum\limits_{x = 1}^{n} \nabla{C_{x}} = \nabla{C}
\end{equation}

\noindent where $\nabla{C_x}$ is the gradient of a single training input and $m$ is also called as mini-batch size.

It is worth noting that approaching a minimum value is proportional to the number of iterations which equals to number of examples in the dataset, therefore; it is a corollary that there can be two situations\footnote{The third case is that algorithm may diverge but we ignore this situation here.}. First, the algorithm cannot reach the minimum point due to lack of training examples, and secondly, the algorithm reaches a global or local minimum point and starts to oscillate around it\cite{zeiler2012adadelta}. There are mechanisms to prevent such cases and other optimization problems like direction sensitivity and they are called as adaptive learning rate \cite{mills2017deep}. In our network we used an algorithm called Adam \cite{kingma2014adam} provided by the framework we used.  

The final step in the gradient descent is updating the weights and biases. We use Eq.~\eqref{eq:NT_Displacement} to perform this task. If we combine Eq.~\eqref{eq:NT_Displacement} with Eq.~\eqref{eq:NT_Stochastic} and write it in open form, the expression to update the weight and bias can be written as;

\begin{equation}
\label{eq:NT_weight_bias_update}
\begin{split}
& \omega^{\prime} = \omega - \frac{\eta}{m} \sum\limits_{j = 1}^{m} \frac{\partial{C_{X_j}}}{\partial{\omega}} \\
& b^{\prime} = b- \frac{\eta}{m} \sum\limits_{j = 1}^{m} \frac{\partial{C_{X_j}}}{\partial{b}}
\end{split}
\end{equation}

\noindent Numerical calculation of the partial derivatives in the equation is one of the costliest computational operation in the training process. To reduce computational cost, there is a well known algorithm called backpropagation \cite{goodfellow2016deep}. We use Pytorch Framework in our study and it provides similar mechanism called automatic differentiation. 

In this section, we reviewd the general concept in artificial neural networks and introduced notions we are going to use in our example. 


\subsection{Dataset and Dataset Generation}

We solved GPE for four different interaction parameters, therefore; there are four different corresponding datasets and each of them contains 10000 elements. We used 8500 of them as training examples and 1500 of them for testing if otherwise is specified. An element of a dataset involves an array containing potential values as a function of position and an another array containing interaction, kinetic, potential and total energy values respectively.

\svgpath{path = {"../figs/dist/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		%\centering
        \includesvg[width=\linewidth]{energy-g-0-}
        \caption{g = 0}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		%\centering
        \includesvg[width=\linewidth]{energy-g-0_1-}
        \caption{g = 0.1}
		\label{fig:b}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        %\centering
        \includesvg[width=\linewidth]{energy-g-1-}
        \caption{g = 1}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        %\centering
        \includesvg[width=\linewidth]{energy-g-10-}
        \caption{g = 10}
		\label{fig:d}
    \end{subfigure}
    \caption{Total energy distributions of the generated solution for different interaction parameter values.}
\label{fig:energy_dist}
\end{figure}


The array containing potential energy is generated according to harmonic trap expression in XMDS, thus; we only supplied angular frequency and shift of equilibrium to the XMDS randomly within predetermined limits. The dimensionless angular frequency can take values between 0.5 and 2. Shift of the equilibrium point is determined due to boundary conditions since density must be zero at the boundaries. In the numerical solution of the GPE, domain of the $z$ dimension is between -10 to 10. Taking the maximum magnitude of the shift as $\mp 5$ is enough to ensure that density function goes to zero at infinity. In both angular frequency and shift, we used uniformly generated random numbers. Total energy distribution is given in Fig~\ref{fig:energy_dist}. Another type distribution is not required in our problem since the potential type is fixed but there are examples such as here \cite{mills2017deep}, which different distribution is used.

%\begin{table}[H]
%\centering
%\caption{CAPTION}
%\label{my-label}
%\begin{tabular}{lllll}
%$\omega$ & 0.5/2         &  &  &  \\
%Shift  & -5/+5         &  &  &  \\
%g      & 0/0.1/1/10/20 &  &  &  \\
%Grid   & 128           &  &  & 
%\end{tabular}
%\end{table}

\section{Machine Learning for GPE}

We used Pytorch Framework \cite{paszke2017automatic} to build our neural networks. It allows the client codes to work on both CPU and GPU via its internal python object called Tensor. If any CUDA supported GPU is available then Pytorch can use GPU without any change in the code. Code have three main parts, first one is dataloaders; it reads train and test data for specified interaction parameter from corresponding file and generates tensor dataset object. In this part, manipulation on dataset can be applied such as normalization, shuffling etc. Second part is implementation of the network. Architecture of the network is represented with a python class inherited from Pytorch's base class for networks called Module. This class also includes a forward method which is responsible for how data will be sent to the next layer. The last part is training and testing. In the training part, the network is iterated with training dataset and loss (result of cost function) is calculated at the end of each iteration. After that, weights and bias are updated accordingly. 

Our aim is to show that the machine learning methods can be used to bypass GPE equation and physical features of the BEC can be determined directly. To do that, we implemented two different types of neural network; fully connected network (FCN) and convolutional neural network (CNN). CNN is more complex than FCN and it is developed to handle data which have grid-like topology \cite{goodfellow2016deep}. Since our dataset involves translation, we expect that CNN can handle such a situation better than FCN.


\subsection{Architecture}

FCN involves 128 input neurons as input layer, next layer is the first hidden layer with 30 neurons, the second is another hidden layer same as the first, the next one is last hidden layer with 10 neurons and the last layer is the output layer. Totally there are 5 layers in our FCN and it will be denoted as $\mathrm{FCN}[128, 30, 30, 10, 4]$. The most common activation function in feedforward networks rectified linear unit (ReLU) \cite{mills2017deep}, is used for each forward feed except output. No operation is applied to the output neuron. Learning rate of the FCN is fixed and it is $0.001$. Cost function is mean squared error (MSE) and optimization is done with Adam. 


CNN has two convolution layers, two maxpool layers and three fully connected layer and the last layer of the fully connected part is the output layer. Maxpooling is applied to output of the first and second convolution layers. ReLu is also applied each forward except output neuron same as in the FCN. Fully connected part of the CNN is $\mathrm{FCN}[310, 100, 20, 1]$. Learning rate, cost function and optimizer of the CNN is same as FCN which are $0.001$, MSE and Adam respectively.


\subsection{Hyperparameters}

We started with small dataset which has 800 elements for training 200 for test to see response of the architecture and to obtain a range for learning rate. The size of this dataset is small but it can give an idea about the range of learning rate since stochastic gradient descent is a statistical approach. We first set learning rate, $\eta$ to 3 and start to decrease it, until 0.01 prediction of the network was extremely poor. For lower values, predictions were more encouraging. In this section, we show the effect of learning rate for two different architectures and interaction parameters. 

\svgpath{path = {"../figs/archtestresults/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
           \centering
            \includesvg[width=\linewidth]{{"fourlayer-g0-lr0_003-800"}}
            \caption{FCN[128, 40, 40, 1], $\eta$ = 0.003}
            \label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
             \includesvg[width=\linewidth]{{"fourlayer-g0-lr0_001-800"}}
            \caption{FCN[128, 40, 40, 1], $\eta$ = 0.001}
            \label{fig:b}
    \end{subfigure}
    \caption{True Energy of Ground State vs Predicted Ground State Energy. Their hyperparameters are identical except the learning rate. Total number of epoch is 20 and batch size is 10.  In this example interaction parameter $g$, is zero. Even the precision of the learning rate 0.001 seems higher, it is trivial to expect that change in interaction parameter will effect the result. }
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
           \centering
            \includesvg[width=\linewidth]{{"fourlayer-g10-lr0_003-800"}}
            \caption{FCN[128, 40, 40, 1], lr = 0.003}
            \label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
             \includesvg[width=\linewidth]{{"fourlayer-g10-lr0_001-800"}}
            \caption{FCN[128, 40, 40, 1], lr = 0.001}
            \label{fig:b}
    \end{subfigure}
    \caption{Here the same network with different interaction parameter which is 10.}
\end{figure}

It is clear that precision of the network with learning rate 0.003 is higher than 0.001. Of course, dramatic increase in the training dataset length may affect them both to converge same precision but our intention here to show that small change in learning rate causes different results.


After determination of learning rate we added extra one hidden layer to the network and tried some different number of neurons combination in the layers.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
           \centering
            \includesvg[width=\linewidth]{fivelayer-g10-800}
            \caption{FCN[128, 30, 30, 10, 1]}
            \label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
             \includesvg[width=\linewidth]{fourlayer-g10-800}
            \caption{FCN[128, 40, 40, 1]}
            \label{fig:b}
    \end{subfigure}
    \caption{Interaction paramater $g$, is 10. Batch size is 10 and total number of epoch is 20 for this example. Precision of the network of 5 layer (a) is higher than 4 layer (b). Bias in (a) can be eliminated by increasing training dataset length.}
\label{fig:network_layer_increment}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{fivelayer-g10-3500}
        \caption{FCN[128, 30, 30, 10, 1]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{fourlayer-g10-3500}
        \caption{FCN[128, 40, 40, 1]}
		\label{fig:b}
    \end{subfigure}
    \caption{Total training dataset length is increased to 3500 and test dataset length is 500. Batch size, Learning rate are same as example given in Fig.~\ref{fig:network_layer_increment} but the epoch is 30. The bias is eliminated. Precision and accuracy of the network of 5 layers has improved.}
\end{figure}

In conclusion, we chose learning rate $\eta$ as 0.001 and started the training process. The other two hyperparameters epoch and mini batch size are determined in the training process. After number of different tries, we set the total number of epochs to 60, and mini batch size to 30. 


\subsection{Training Results}

We give the result of total energy predictions per 20 epochs for both FCN and CNN also to show how predictions change. Results of $\mathrm{FCN}[128, 30, 30, 10, 4]$ used in prediction of interaction, potential, kinetic and total energy separately are given directly. For the inverse problem, three different scenarios are presented. All trainings are done with identical networks and only the training and test datasets are changed. 

\subsubsection{Non-interacting System}

In non-interacting systems, GPE reduces to SE which does not involve non-linear term. 

\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		%\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		%\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        %\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        %\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:d}
    \end{subfigure}
    \caption{Result of FCN[128, 30, 30, 10, 1] for non-interacting system.}
\label{fig:FFN-g-0}
\end{figure}

In figures, $x$ axis represents true dimensionless total energy values, and $y$ is predicted energy by trained network. Inset histogram at the left upper corner is relative error given as percentage and the inset histogram at the right corner is difference error.

Since the system has no interaction parameter, problem is relatively easy when it is compared to systems that involve interaction. In Fig.~\ref{fig:FFN-g-0}(a) it is obvious that the network requires more training examples and it reaches a satisfactory level in Fig.~\ref{fig:FFN-g-0}(b). Supplying another 20 more epochs reduces both relative error and MSE as shown in Fig.~\ref{fig:FFN-g-0}(d)


\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0-conv1d-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0-conv1d-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0-conv1d-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0-conv1d-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
    \caption{CNN results for $g = 0$}
\label{fig:CNN-g-0}
\end{figure}

The bias in Fig.~\ref{fig:FFN-g-0}(a) does not appear in Fig.~\ref{fig:CNN-g-0}(a). One reason why there is less or no bias in CNN predictions could be because CNN's have the ability to handle translations in dataset. In our dataset, translation corresponds to shift of equilibrium point. 

\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-E_int-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-E_pot-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-E_kin-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-E_Total-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{FCN[128, 30, 30, 10, 4] results for $g = 0$. Here (a) is interaction energy, (b) potential energy, (c) kinetic energy and (d) total energy predictions.}
\label{fig:FFN-g-0-S}
\end{figure}

Only number of neurons in the output layer is increased to 4, and still network predicts total energy of the system within same sensitivity as former FCN network.


\subsubsection{Interacting Systems}

In interacting systems, values of interaction parameter $g$ are 0.1, 1, 10, 20 and the results are given in this order. In this stage, the values of the interaction parameter $g$ can be thought directly as a coefficient of a nonlinear partial differential equation rather than its physical meaning. In $g =1$ scenario, contribution of interaction parameter to Eq.~\eqref{eq:GPE_dimensionless} will be more vivid. (We do not take into account the attractive interaction case which is represented by negative $g$. In such a situation, BEC collapses \cite{barenghi2016primer}.)

\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{FCN[128, 30, 30, 10, 1] results for $g = 0.1$.}
\label{fig:FFN-g-0.1}
\end{figure}

In first 20 epoch Fig.~\ref{fig:FFN-g-0.1}(a), there are relatively higher deviations in lower energies. In Fig.~\ref{fig:FFN-g-0.1}(b), deviations become smaller but still visible. In Fig.~\ref{fig:FFN-g-0.1}(c), prediction line is smoother but still deviations remains in lower energy levels.

Although the accuracy of the network is high, loss figure Fig.~\ref{fig:FFN-g-0.1}(d) still has fluctuation.

\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-conv1d-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-conv1d-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-conv1d-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-conv1d-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{CNN results for $g = 0.1$}
\label{fig:CNN-g-0.1}
\end{figure}

In Fig.~\ref{fig:CNN-g-0.1}(a) deviation has nearly same characteristic with FCN in lower energies. After 60 epochs Fig.~\ref{fig:CNN-g-0.1}(c), there is a visible shift in higher energies but this does not occur in Fig.~\ref{fig:CNN-g-0.1}(b). Local peeks can be seen on Fig.~\ref{fig:CNN-g-0.1}(d) which increases error. Different techniques may be applied to recover the state at Fig.~\ref{fig:CNN-g-0.1}(b). For instance, before final test, network can check itself and compare different losses at different epochs and can select the minimum case. 


\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-E_int-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-E_pot-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-E_kin-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-E_Total-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{Separate Energy Predictions for $g = 0.1$ }
\label{fig:FFN-g-0.1-S}
\end{figure}


Output layer of the network involves four layer and since our network is fully connected, some of the weights at previous layers are shared indirectly. The order of interaction energy is much smaller than the others. This is the reason of bias in Fig~\ref{fig:FFN-g-0.1-S}(a). Because, the order of incoming inputs are same as other energy levels and network has not enough example to reduce weights and bias values in order to increase prediction accuracy. Applying a proper normalization to the variables may reduce the error. In this time, incoming inputs will be in the same order.


\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{FCN[128, 30, 30, 10, 1] results for $g = 1$}
\label{fig:FFN-g-1}
\end{figure}

Relative error gets smaller after each 20 epoch and network goes into saturation after 40 epoch.

\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-conv1d-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-conv1d-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-conv1d-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-conv1d-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{CNN results for $g = 1$}
\label{fig:CNN-g-1}
\end{figure}

Even the loss of the CNN decreases slower than FCN Fig.~\ref{fig:FFN-g-1}(d) and involves local peeks, its relative error at epoch 40 and 60 is smaller. 

\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-E_int-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-E_pot-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-E_kin-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-E_Total-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{FCN[128, 30, 30, 10, 4], Separate energy predictions for $g = 1$.}
\label{fig:FFN-g-1-S}
\end{figure}

This is the worst case in our training results. In first sight, it can be thought that a proper normalization may reduce the error and increase the accuracy but the difference between energy values are not as higher as in the case $g = 0.1$, so the problem is not due to normalization. Here, contribution of interaction parameter to Eq.~\eqref{eq:GPE_dimensionless} is in same order with other terms. So, it can be said that the degree of freedom of the network is not enough to handle such a situation.

\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{FCN[128, 30, 30, 10, 1] predictions for $g = 10$}
\label{fig:FFN-g-10}
\end{figure}

In Fig.~\ref{fig:FFN-g-10}(a) same deviation occurs in lower and higher energies as in previous results. Loss of the network decreases rapidly but order of it greater than the case of which $g = 1$


\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-conv1d-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-conv1d-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-conv1d-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-conv1d-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{CNN results for $g = 10$}
\label{fig:CNN-g-10}
\end{figure}

Accuracy and precision of the CNN are greater than FCN's, both relative error and MSE are smaller than FCN predictions. Also unlike FCN, there is no bias in first 20 epochs. Another important subject is that last 20 epochs increases MSE but reduces relative error slightly. 

\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-E_int-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-E_pot-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-E_kin-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-E_Total-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{Separate prediction  g=10}
\label{fig:FFN-g-10-S}
\end{figure}

Result of separate energy prediction for $g=10$ is not poor as $g = 1$. This also shows that dominance of a term in the equation directly affects the prediction. 

\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{FCN[128, 30, 30, 10, 1] predictions for g = 20}
\label{fig:FFN-g-20}
\end{figure}

In Fig.~\ref{fig:FFN-g-20}(d), the network goes into saturation before 10 epochs. This shows that how the arrangement of the dataset and initialization of the network affects the predictions. A shuffling  on both dataset and network initialization may produce different loss graph. 

\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-conv1d-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-conv1d-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-conv1d-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-conv1d-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:d}
    \end{subfigure}
	\caption{CNN predictions for $g = 20$}
\label{fig:CNN-g-20}
\end{figure}

Unbiased predictions in first 20 epochs becomes biased after 40 epochs. It is because of energy distribution in the dataset. The number of examples at higher energies is greater than number of lowers. 

\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-E_int-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-E_pot-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-E_kin-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-E_Total-}
        \caption{FCN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{Separate predictions for g=20}
\label{fig:FFN-g-20-S}
\end{figure}

The problem in Fig.~\ref{fig:FFN-g-20-S}(c) is same as in the case $g = 0.1$ which is shown in Fig.~\ref{fig:FFN-g-0.1-S}(a). The order of kinetic energy is  lower than other energies. 

\subsubsection{Inverse Problem Prediction of ${g}$}

In this section, we try to predict the interaction parameter for the density. We accept that $g$ is positive and the value of it can vary between 0 to 10. We used modified version of CNN to predict interaction parameter. Modification only involves a change in input layer of the network. We increased the number of channels to 2 and one of the channels represents potential the other is density. Dataloaders also updated to handle new input representation. 

There are three different cases. The first one is that both potential and shift are fixed, only the interaction parameter varies. In the second one we used random angular frequencies. In final case, we both randomized angular frequency and shift.


\svgpath{path = {"../figs/CNNgPrediction/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
    	\includesvg[width=\linewidth]{fixed-pot_dens-g-VARY-conv1d-epoch-30-}
    	\caption{Epoch = 30}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
    	\includesvg[width=\linewidth]{various-pot_dens-g-VARY-conv1d-epoch-30-}
    	\caption{Epoch = 30}
		\label{fig:b}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
		\includesvg[width=\linewidth]{various-shift-pot_dens-g-VARY-conv1d-epoch-30-}
    	\caption{Epoch = 30}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
		\includesvg[width=\linewidth]{FPFS-VPFS-VPVS-LOSS-}
    	\caption{Loss}
		\label{fig:d}
    \end{subfigure}

	\caption{In (a) is angular frequency is fixed and there is no shift (FPFS). In (b) angular frequency is randomized and no shift (VPFS). In (c) both angular frequency and shift are randomized (VPVS).}
\label{fig:FFN-g-pred}
\end{figure}

When there is no randomization in parameters the network handles the situation easily but when angular frequency and shift starts to vary, the network produces poorer results. It is an expected situation and it does not show that the network is not capable of to predict interaction parameter. As shown in Fig.~\ref{fig:FFN-g-pred}(c), precision of the network is not good but the accuracy is. Expansion in the training dataset and increase in total number of epochs will fix the problem.

\clearpage
\section{Conclusion}

In this study, we implemented an artificial neural network to show a proof of concept that machine learning methods can also be applied to non-linear Schr{\"o}dinger equation. Our results show that when the harmonic trapping potential is given, ground state energy of the Bose-Einstein can be obtained without solving the corresponding Gross-Pitaevskii equation. We also managed to predict interaction, kinetic, potential and total energy of a system separately. Error in our results differs case to case. The worst result occurred in separate energy predictions when the interaction parameter is $1$ and the relative error came out about $\%10$. Because of the contribution of the interaction parameter to the Gross-Pitaevskii equation, all terms become same order and the degree of freedom of the network is not enough to make predictions. However, in most cases except separate energy predictions relative error is lower than $\%2$. In inverse problem, we built another neural network to predict interaction parameter when potential and density are given. The prediction accuracy decreases when angular frequency of the trapping potential and shift of equilibrium point is not fixed but mean of the relative error for it is $1.15$.

Our work only involves harmonic trapping potential and limited interaction parameters. Each trained network only accept inputs for a fixed interaction parameter. Our future work will focus on more generic situations such that the network will be able to predict ground state energy of a condensate when both potential and interaction parameter is random. It is also possible to work with two dimensional trapping potential. Our aim is first successful implementation of random interaction parameter and one dimensional random potentials. After that, we will try two dimensional case. 

\clearpage
\bibliographystyle{ieeetr}
\bibliography{references}


\appendix
\section{APPENDIX A}

\subsection{Comment on Python Codes}

There are two neural network codes and three utility codes. Code of FCN is feedforwardnetwork.py, CNN is cnnnetwork1d.py. analyzer.py is kind of a network tracer. All information about network internals and iteration information is saved and traced by class inside this module. readdata.py reads the data generated by XMDS and produces proper representations of the data. sampletrainloader.py takes the data from readdata.py and produces Pytorch's tensors. run\_training.py automatizes the training process. 


Codes are not optimized and consistency of the naming convention is poor. But volume of the code is small and it is self explanatory. Code can be obtained from: goo.gl/8Fm8yt


%\subsection{Neural Network and Utility Codes}
%
%\setminted{fontsize=\footnotesize,baselinestretch=1}
%\textbf{Fully Connected Network (FCN)}\\
%(feedforwardnetwork.py)
%\inputminted[breaklines]{python}{../Src/feedforwardnetwork.py}
%
%\noindent \textbf{Convolutional Neural Network (CNN)}\\
%(cnnnetwork1d.py) 
%\inputminted[breaklines]{python}{../Src/cnnnetwork1d.py}
%
%\noindent \textbf{Utility modules:}\\
%(analyzer.py)
%\inputminted[breaklines]{python}{../Src/analyzer.py}
%
%\noindent (readdata.py)
%\inputminted[breaklines]{python}{../Src/readdata.py}
%
%\noindent (sampletrainloader.py)
%\inputminted[breaklines]{python}{../Src/sampletrainloader.py}
%
%\noindent (run\_trainings.py)
%\inputminted[breaklines]{python}{../Src/run_trainings.py}
%
%\newpage

\end{document}