\documentclass[a4paper,times,12pt]{article}
\usepackage{amsmath}
%\usepackage{graphicx}
\usepackage{float}
\usepackage{setspace}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{minted}
\usepackage[multidot]{grffile}

\onehalfspacing
\usepackage[top=2.5 cm, bottom=2.5 cm, left=4 cm, right=2.5 cm]{geometry}
\usepackage{hyperref}

%\title{\textbf{TEST}}
%\date{}

\begin{document}


%\maketitle
\setcounter{page}{1}
\pagenumbering{arabic}

\section*{Summary}

We train an artificial neural network to estimate the ground state energy
of a one-dimensional Bose-Einstein condensate in harmonic trapping potential.
Such a system can be described by the solution of a corresponding
Gross-Pitaevskii equation also called a non-linear Schrodinger equation.
We also use the method to predict the non-linearity parameter using the ground
state density profile for a given harmonic trapping potential.


\section{Introduction}
\section{Gross Pitaevskii Equation}

A Bose-Einstein Condensate (BEC) is described by Gross Pitaevskii Equation (GPE) also known as non-linear Schrodinger Equation. General form of GPE is given as,

\begin{equation}
\label{eq:GPE_3D}
i \hbar \frac {\partial \Psi}{\partial t} = \frac {-\hbar}{2m}\nabla^2
\Psi + V(\boldsymbol{r}, t)\Psi + g|\Psi|^2\ \Psi
\end{equation}


where %%
where $\hbar$ is Planck constant, $\boldsymbol{r}$ is position vector, $t$ is time, $\Psi$ is wave function, $m$ is mass, $\nabla^2$ is Laplacian, $V$ is potential, $g$ is interaction parameter and it is defined as 

\begin{equation}
\label{eq:GPE_inter_param}
g=\frac{4\pi\hbar^2a_s}{m}
\end{equation}

where $a_s$ s wave scattering length. Non-linearity of GPE comes from interaction parameter. If there is no interaction GPE reduces to the Schrodinger Equation (SE) and becomes linear. By definition, $g$ can be positive or negative. If $g > 0$, it represents repulsive interaction, and if $g < 0$, it means interaction is attractive. \textbf{REF}. 

In general, potential $V$ is time independent harmonic trapping potential \textbf{REF}. In such a case, the potential is only function of position $V(\boldsymbol{r})$ and it can be given as,
\begin{equation}
\label{eq:GPE_harmonic_potential}
V(x, y, z) = \frac{1}{2}m(\omega_x^2 x^2 + \omega_y^2y^2 + \omega_z^2z^2)
\end{equation}


where $\omega$ is angular frequency. Then separation of variables can be applied and it can be obtained that there is a solution in stationary form. In this case factorized wave function is,

\begin{equation}
\label{eq:GPE_time_indep_wave_func}
\Psi(\boldsymbol{r}) = \psi(\boldsymbol{r})e^{-i\mu t/\hbar}
\end{equation}

Here, $\mu$ is chemical potential. Time independent GPE is then,

\begin{equation}
\label{eq:GPE_time_indep}
\mu\psi = \frac{-\hbar^2}{2m}\nabla^2\psi + V(\boldsymbol{r})\psi + g|\psi|^2\psi 
\end{equation}

BEC system can also be studied in one dimension such that making angular frequencies $\omega_x, \omega_y \gg \omega_z$ and keeping their energy order much greater than condensate's energy $ \hbar(\omega_x \omega_y)^{1/2} \gg \mu $ enables us to confine the dynamics of the system in one dimension and it can be described with corresponding 1D GPE equation. \textbf{REF}. To obtain 1D GPE one can rewrite wave function,

\begin{equation}
\label{eq:GPE_time_indep_wave_func_seperated}
\psi(x, y, z, t) = \psi_z(z, t)\psi_0(x)\psi_0(y)
\end{equation}

\textbf{continue}

\begin{equation}
\label{eq:GPE_1D}
\mu^{\prime}\psi_z = \frac{-\hbar^2}{2m}\frac{d^2\psi_z}{dz^2} + \frac{1}{2}m\omega_z^2 z^2\psi_z + g^{\prime}|\psi_z|^2\psi_z 
\end{equation}

where $\mu^{\prime}$ and $g^{\prime}$ one dimensional effective chemical potential and interaction strength respectively and they are given as,

\begin{equation}
\label{eq:GPE_1D_chem_inter}
\mu^{\prime} = \mu - \frac{\hbar}{2}(\omega_x - \omega_y), \quad g^{\prime} = \frac{g}{2\pi l_x l_y}
\end{equation}


\textbf{Dimensionless GPE}\\

It is convenient to work with dimensionless quantities, to make equation dimensionless one can define $z = a_0\widetilde{z}$. In this case Eq.~\ref{eq:GPE_1D} becomes,

\begin{equation}
\label{eq:GPE_dimensionless_length}
\mu\psi = \frac{-\hbar^2}{2ma_0^2}\frac{d^2\psi}{d\widetilde{z}^2} + \frac{1}{2}m\omega^2 a_0^2 \widetilde{z}^2\psi + g|\psi|^2\psi
\end{equation}

(Here we omit scripts). After that, setting $\hbar\omega = {\hbar^2}/{m a_0^2}$, and plugging $a_0^2$ in to the Eq.~\ref{eq:GPE_dimensionless_length},

\begin{equation}
\label{eq:GPE_dimensionless_energy}
\mu\psi = -\frac{1}{2}m\omega^2\frac{d^2\psi}{d\widetilde{z}^2} + \frac{\hbar^2}{2m}\widetilde{z}^2\psi + g|\psi|^2\psi
\end{equation}

If we divide Eq.~\ref{eq:GPE_dimensionless_energy} to $\hbar\omega$ and define $\widetilde{\mu} = \mu/\hbar\omega$,

\begin{equation}
\label{eq:GPE_dimensionless_without_g}
\widetilde{\mu} \psi = -\frac{1}{2\sqrt{a_0}}\frac{d^2\psi}{d\widetilde{z}^2} + \frac{1}{2\sqrt{a_0}}\widetilde{z}^2\psi + \frac{g|\psi|^2\psi}{\hbar\omega}
\end{equation}

From normalization condition,

\begin{equation}
\int |\psi|^2dz = \int |\psi|^2 a_0 d\widetilde{z} = N
\end{equation}

we can define $\widetilde{\psi} = ({\sqrt{a_0}}/{\sqrt{N}})\psi$. The third term on the right in Eq.~\ref{eq:GPE_dimensionless_without_g} becomes,

\begin{equation}
\label{eq:GPE_dimensionless_g_1}
\frac{g|\psi|^2}{\hbar\omega} = \frac{g a_0}{N\hbar\omega}|\widetilde{\psi}|^2 
\end{equation}

Finally defining,

\begin{equation}
\label{eq:GPE_dimensionless_g_2}
\frac{g a_0}{N\hbar\omega_z} = \widetilde{g}
\end{equation}

dimensionless GPE can be written as,

\begin{equation}
\label{eq:GPE_dimensionless}
\widetilde{\mu} \widetilde{\psi} = -\frac{1}{2}\frac{d^2\widetilde{\psi}}{d\widetilde{z}^2} + \frac{1}{2}\widetilde{z}^2\widetilde{\psi} + \widetilde{g}|\widetilde{\psi}|^2 \widetilde{\psi}
\end{equation}



\subsection{Types of Potentials}


\subsection{Analytical Solution and Approximation}

There is no general analytical solution of GPE for harmonic trapping potential except the case which interaction parameter is zero. In this case, GPE reduces to the SE and solution is well known.


\subsection{Numerical Solution and XMDS Framework}

Analytical solution of GPE is known for only few cases such as uniform condensate which is $V = 0$. Most of the time GPE is solved with numerical techniques or approximations. In this study, we used a framework called XMDS, implemented specifically to solve differential equation systems with well optimized numerical methods. Equation system can be described by a markup language called XML.  When equation system is declared properly, XMDS produces a source code written in C++ that solves the equation with pre-specified numerical method. In our program, XMDS takes angular frequency, shift of equilibrium point, value of interaction parameter and number of particles in the system externally. It generates trapping potential and other quantities internally with supplied expressions.

Cross-check of the framework has done in two ways. First, since there exists an analytical solution for $g = 0$, we compared the results of the XMDS with analytical ones, and we also used the program here \textbf{REF}, and compared few results for situations where analytical solution does not exist.  


\section{Problem Statements and Dataset Generation}

\textbf{GPE can be solved with stated methods above. These methods must be applied every time when there is a change in the equation such as different trapping potential or parameters.} For example, when interaction parameter is changed, if there is no analytic solution, numerical method must be reapplied. With machine learning these steps can be bypassed and information about the system can be obtained instantly with only one time cost which is training process of the network. \textbf{REF}. Here \textbf{REF}, authors managed to apply deep learning method to the Schrodinger Equation to obtain ground state energy of the system under different potentials. 

We try to built a artificial neural network to predict ground state energy of a BEC for a given harmonic trapping potential and interaction parameter. We also try inverse problem which is prediction of interaction parameter for a given potential and density function. 

An artificial neural network is made of layers and these layers contains simple units called neurons. These neurons can be thought as primitive version of the neurons in the human brain. A neuron takes one or more input and generates an output. To do this, it uses its internal variables which are called weight and bias. In general, number of weights in a neuron is equal to the input size of the neuron and there is only one bias value per neuron. There is no restriction to range of weights and biases except computational, they can even take complex values {\textbf{REF}}. Input output relation of a neuron can be described with the following way; let $\boldsymbol{x} = [x_1, x_2, x_3, ..., x_n]$ be input of the neuron and $f$ be the function that describes behavior of the neuron. 

\begin{equation}
\label{eq:NU_neuron}
f(\boldsymbol{x}, \boldsymbol{\omega}, b) = \sum\limits_{i = 1}^n x_i\omega_i + b
\end{equation}

Here, $\omega_i$ is weight and $b$ is bias. A layer of a network involves neurons behaves like this function. In our example, $x_n$ inputs represent the potential expression respect to the position or density function. \textbf{FIGURE OF NETWORK and explanation}. Notice that, the range of the function $f$ is infinite. There are various ways to interpret result of function $f$. The most primitive version of interpretation is sending the result of $f$ as a argument to unit step function. In this case, 

\begin{equation}
\label{eq:NU_step_function}
a = u_1(f(\boldsymbol{x}, \boldsymbol{\omega}, b))
\end{equation}

where $a$ is called as activation value of neuron and $u_1$ is the activation function. However, usage of the step function makes the network discontinuous and does not allow to use differential methods to calculate behavior of the neurons when a small change in weights or biases is applied. One of the most common activation function is sigma function and it is defined as,

\begin{equation}
\label{eq:NU_sigma_function}
\sigma(f) = \frac{1}{1 + e^{-f}}
\end{equation}

If a neuron defined with this way such that its output given by Eq.~\ref{eq:NU_sigma_function}, then it is called as sigmoid neuron. In former case which is step function, then, the neuron is called as perceptron. 

If we denote $j^{th}$ neuron in the $l^{th}$ layer with $f_j^l$, and the weight from $k^{th}$ in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the layer $l^{th}$ with $\omega_{jk}^l$, finally the bias of the $j^{th}$ neuron in the $l^{th}$ layer with ${b_j^l}$, then, the connection between two neuron in sequential layers will be;

\begin{equation}
\label{eq:NU_neuron_connection}
f_s^{l+1}(\sigma(f_j^{l}(\boldsymbol{x}, \boldsymbol{\omega}, b)), \boldsymbol{\omega}, b) = \sum\limits_{i=1}^{n_2} \sum\limits_{j=1}^{n_1} (x_j\omega_j + b_1)\omega_i + b_2
\end{equation}
\textbf{Eq requires correction}

Here we used $\sigma$ as activation function but it can be any suitable activation function such as hyperbolic tangent $tanh(f)$.  

Connection complexity of the neurons may vary. Output of each neuron in one layer can be input of every neuron in next layer. Such a networks are called Fully Connected Networks. \textbf{REF}. If result of one layer is directly sent to the next layer without any circulation or feedback, then the network is called as Feed Forward Network. \textbf{REF}

We are going to change these weights and biases in such a way that the difference between output generated by network and real value of the corresponding quantity will be minimized. To do that, we are going to use a proxy relation between output and real value which is called as Cost Function. \textbf{REF}. There are many cost functions such as quadratic cost function, cross entropy cost function etc. We are going to use quadratic cost function also called as mean squared error to show general mechanism and to introduce few notions that directly effects the behavior of the network. 

Quadratic cost function is defined as;

\begin{equation}
\label{eq:NT_Quadratic}
C(\omega, b) = \frac{1}{2n} \sum\limits_{x} || \boldsymbol{a} - f_L(\boldsymbol{x}) ||{^2} 
\end{equation}

where $\omega$, $b$ are weights and biases respectively. $f_L(\boldsymbol{x})$ output produced by network and $\boldsymbol{a}$ is real value of the quantity. To minimize $C$, we can take the derivative of the function and can find extremum values, but this method is extremely costly because the total number of neurons in the network is enormous. \textbf{REF}. This problem is overcame by an iterative algorithm called Gradient Descent. Since $C$ is also a scalar field the algorithm tries to determine a direction to which points the decrement and moves to that direction with small steps. In each iteration, algorithm repeats itself to reach minimum value. 

A small displacement in arbitrary direction which corresponds to small change in weight or bias or both can be written as;
\begin{equation}
\label{eq:NT_Quadratic_min}
\Delta{C} = \frac{\partial{C}}{\partial{\omega}}\Delta{\omega} + \frac{\partial{C}}{\partial{b}}\Delta{b}
\end{equation}

This expression gives information about what happens when small displacement is done but it does not give any information about direction of decrement. To determine direction of decrement we can rewrite this expression in terms of gradient since it is by definition points to direction of maximum rate of increase. 

We can define gradient operator as;

\begin{equation}
\label{eq:gradient}
\boldsymbol{\nabla}{C} = \left(\frac{\partial{C}}{\partial{\omega}},\frac{\partial{C}}{\partial{b}} \right)^T
\end{equation}

In this case, Eq.~\ref{eq:NT_Quadratic_min} can be written as;

\begin{equation}
\label{eq:NT_Quadratic_min_gradient_form}
\Delta{C} = \boldsymbol{\nabla}{C} \boldsymbol{.} \Delta \boldsymbol{l}
\end{equation}

where $\boldsymbol{l} = (\omega, b)^T$. What we want again is to minimize cost function, thus; in each iteration value of the C must be smaller than previous one which is equally saying that $\Delta{C}$ must be smaller than zero, therefore; direction of displacement must be in the opposite direction of gradient. Rather than calculating the opposite direction on each iteration we can define $\Delta{\boldsymbol{l}}$ in such a way so that it always points to opposite direction of the gradient.

\begin{equation}
\label{eq:NT_learning_rate}
\Delta{\boldsymbol{l}} = -\eta\boldsymbol{\nabla}C
\end{equation}

In this case, Eq.~\ref{eq:NT_Quadratic_min_gradient_form} becomes;

\begin{equation}
\label{eq:NT_Displacement}
\Delta{C} = -\eta ||\boldsymbol{\nabla}{C}||^2
\end{equation}

where $\eta$ is positive definite number and it is called as learning rate. Since it is guaranteed that $||\boldsymbol{\nabla}{C}||^2 \geq 0$, therefore; $\Delta{C} \leq 0$. In this situation, learning rate is the step size in each iteration. From Eq.~\ref{eq:NT_Displacement} it is intuitive that $\eta$ can not be a large number because in such a case, one can miss the minimum point. \textbf{FIGURE}. This situation also brings up the subject that gradient descent does not give guarantee to find the minimum point.{\textbf{REF}} If there is no enough example to iterate gradient descent, cost function may be lower than its first condition but it will not be minimized. \textbf{It may even retain its first condition ex:lr=0.05}. 

\subsection{Dataset and Dataset Generation}

As mentioned, we solved GPE for four different interaction parameters, therefore; there are four different corresponding datasets and each of them contains 10.000 elements. We used 8500 of them as training examples and 1500 of them for test if otherwise is specified. An element of a dataset involves an array containing potential values respect to the position and an another array containing interaction, kinetic, potential and total energy values respectively. (Representation of the energy values may vary, because we first tried to predict total energy values. In this case, the array containing all types of energy values only involves total energy).

\textbf{TABLE of parameters}


\svgpath{path = {"../figs/dist/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		%\centering
        \includesvg[width=\linewidth]{energy-g-0-}
        \caption{g = 0}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		%\centering
        \includesvg[width=\linewidth]{energy-g-0_1-}
        \caption{g = 0.1}
		\label{fig:b}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        %\centering
        \includesvg[width=\linewidth]{energy-g-1-}
        \caption{g = 1}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        %\centering
        \includesvg[width=\linewidth]{energy-g-10-}
        \caption{g = 10}
		\label{fig:d}
    \end{subfigure}
    \caption{Histograms represents total energy distributions for different interaction parameter values.}
\label{fig:energy_dist}
\end{figure}


The array containing potential energy is generated according to harmonic trap expression in XMDS, thus; we only supplied angular frequency and shift of equilibrium to the XMDS randomly within predetermined limits. Angular frequency can take values between 0.5 and 2. Shift of the equilibrium point is determined due to boundary conditions since \textbf{density function} must be zero at boundaries. In numerical solution of the GPE, domain of the $z$ dimension is between -10 to 10. Taking the maximum magnitude of the shift as $\mp 5$ enough to ensure that density function goes to zero at infinity. In both angular frequency and shift, we used uniformly generated random numbers. Total energy distribution is given in Fig~\ref{fig:energy_dist}. Another type distribution is not required in our problem since the potential type is fixed but there are examples such as here \textbf{REF}, which different distribution is used.

\section{Machine Learning for NLSE}

We used Pytorch Framework to build our neural networks. It allows the client codes work on both CPU and GPU via its internal python object called Tensor. If any CUDA supported GPU is available then Pytorch can use GPU without any change in the code. Code have three main parts, first one is dataloaders; it reads train and test data for specified interaction parameter from corresponding file and generates tensor dataset object. In this part, manipulation on dataset can be applied such as normalization, shuffling etc. Second part is implementation of the network. Architecture of the network is represented with a python class inherited from Pytorch's base class for networks called Module. This class also involves forward method which is responsible for how data will be sent to the next layer. 


We implemented two different types of neural network; feed forward (FNN) and convolutional neural network (CNN).


\subsection{Architecture}

FNN involves 128 input neurons as input layer, next layer is first hidden layer with 30 neurons, the second is same as first hidden layer, the next one is last hidden layer with 10 neurons and the last layer is output layer. Totally there are 5 layers in our FFN and it will be denoted as $FNN[128, 30, 30, 10, 4]$. Rectified linear unit (ReLU) is used for each forward except output. No operation is applied to the output neuron. Learning rate of the FNN is fixed and it is $0.001$. Cost function is mean squared error (MSE) and optimization is done with Adam. 


CNN has two convolution layers, two maxpool layers and three fully connected layer and last layer of the fully connected part is output layer. Maxpooling is applied to output of the first and second convolution layers. ReLu is also applied each forward except output neuron same as in the FNN. Fully connected part of the CNN is $FNN[310, 100, 20, 1]$. Learning rate, cost function and optimizer of the CNN is same as FNN which are $0.001$, MSE and Adam respectively.


\subsection{Hyperparameters}

We firstly started with smaller dataset which involves 800 elements for trainig 200 for test to see response of the architecture and to obtain a range for learning rate. The size of this dataset is \textbf{very small} but it can be give an idea about the range of learning rate since stochastic gradient descent is an statistical approach. \textbf{REF}. We first set learning rate 3 and decrease it, until 0.01 prediction of the network was extremely poor. For lower values, predictions were more encouraging. In this section, we give you the effect of learning rate for two different architectures and interaction parameters. 

\svgpath{path = {"../figs/archtestresults/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
           \centering
            \includesvg[width=\linewidth]{{"fourlayer-g0-lr0_003-800"}}
            \caption{FNN[128, 40, 40, 1], $\eta$ = 0.003}
            \label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
             \includesvg[width=\linewidth]{{"fourlayer-g0-lr0_001-800"}}
            \caption{FNN[128, 40, 40, 1], $\eta$ = 0.001}
            \label{fig:b}
    \end{subfigure}
    \caption{Here both figures represent True Energy of Ground State (unit) vs Predicted Ground State Energy (unit). Their hyperparameters are identical except learning rate. Total number of epoch is 20 and batch size is 10.  In this example interaction parameter $g$, is zero. Even learning rate 0.001 seems more precise, it is trivial to expect that change in interaction parameter will effect the result. }
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
           \centering
            \includesvg[width=\linewidth]{{"fourlayer-g10-lr0_003-800"}}
            \caption{FNN[128, 40, 40, 1], lr = 0.003}
            \label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
             \includesvg[width=\linewidth]{{"fourlayer-g10-lr0_001-800"}}
            \caption{FNN[128, 40, 40, 1], lr = 0.001}
            \label{fig:b}
    \end{subfigure}
    \caption{Here, interaction parameter, g is 10. It is clear that precision of the network with learning rate 0.003 is higher than 0.001. Of course, dramatic increase in the training dataset length may affect them both to converge same precision but our intention here to show that small change in learning rate causes different results.}
\end{figure}

After determination of learning rate we added extra one hidden layer to the network and tried some different number of neurons combination in the layers.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
           \centering
            \includesvg[width=\linewidth]{fivelayer-g10-800}
            \caption{FNN[128, 30, 30, 10, 1]}
            \label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
             \includesvg[width=\linewidth]{fourlayer-g10-800}
            \caption{FNN[128, 40, 40, 1]}
            \label{fig:b}
    \end{subfigure}
    \caption{Here interaction paramater $g$, is again 10. Batch size is 10 and total number of epoch is 20 for this example. Precision of the network of 5 layer (a) is higher than 4 layer (b). Bias in (a) can be eliminated by increasing training dataset length.}
\label{fig:network_layer_increment}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{fivelayer-g10-3500}
        \caption{FNN[128, 30, 30, 10, 1]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{fourlayer-g10-3500}
        \caption{FNN[128, 40, 40, 1]}
		\label{fig:b}
    \end{subfigure}
    \caption{Total training dataset length is increased to 3500 and test dataset length is 500. Batch size, Learning rate are same as example given in Fig.~\ref{fig:network_layer_increment} but the epoch is 30. The bias is eliminated. Precision and accuracy of the network of 5 layers is sufficient.}
\end{figure}

In conclusion, we chose learning rate $\eta$ as 0.001 and started to training process. The other two hyperparameters epoch and mini batch size determined in training process such that if increase in total number of epochs improves the results dramatically then new epoch value is this one and the mini batch size is determined with the same way. These steps are done for both FFN and CNN. The total number of epochs is 60, and mini batch size is 30. 


\subsection{Training Results}

We give the result of total energy predictions per 20 epochs for both FFN and CNN also to show that how predictions change. Separate energy predictions are given directly. In inverse problem, three different scenarios are presented. All trainings are done with same programs, thus; the architectures are identical, only the training and test datasets are changed. 

\subsubsection{Non-interacting System}

In non-interacting systems, GPE reduces to SE which does not involve non-linear term. 

\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		%\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		%\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        %\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        %\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:d}
    \end{subfigure}
    \caption{In figures, $x$ axis represents true dimensionless total energy values, and $y$ is predicted energy by trained network.}
\label{fig:FFN-g-0}
\end{figure}

Since the system has no interaction parameter, problem is relatively easy when it is compared to systems that involve interaction. In Fig.~\ref{fig:FFN-g-0}(a) it is obvious network requires more training example and it reaches a satisfactory level in Fig.~\ref{fig:FFN-g-0}(b). Supplying another 20 more epochs nearly does not effect the prediction accuracy shown in Fig.~\ref{fig:FFN-g-0}(d) network went into saturation.


\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0-conv1d-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0-conv1d-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0-conv1d-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0-conv1d-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
    \caption{CNN results for $g = 0$}
\label{fig:CNN-g-0}
\end{figure}

The bias in Fig.~\ref{fig:FFN-g-0}(a) does not appear in Fig.~\ref{fig:CNN-g-0}(a). The reason why there is less or no bias in CNN predictions is because CNN's have the ability to handle translations in dataset. In our dataset, translation corresponds to shift of equilibrium point. 

\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-E_int-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-E_pot-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-E_kin-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0-epoch-60-E_Total-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{Here (a) is interaction energy, (b) potential energy, (c) kinetic energy and (d) total energy predictions. Only number of neurons in the output layer is increased to 4, and still network predicts total energy of the system within same sensitivity as former FNN network.}
\end{figure}



\subsubsection{Interacting Systems}

In interacting systems values of interaction parameter $g$ are 0.1, 1, 10, 20 and the results are given in this order. 

\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-20-}
        \caption{FNN[128, 30, 30, 10, 1]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-40-}
        \caption{FNN[128, 30, 30, 10, 1]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-}
        \caption{FNN[128, 30, 30, 10, 1]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-LOSS-}
        \caption{FNN[128, 30, 30, 10, 1]}
		\label{fig:c}
    \end{subfigure}
	\caption{In first 20 epoch (a), there are relatively higher deviations in lower energies. In (b), deviations become smaller but still visible. In (c), prediction line is smoother but still deviations remains in lower energy levels.}
\label{fig:FFN-g-0.1}
\end{figure}

Although the accuracy of the network is high, loss figure Fig.~\ref{fig:FFN-g-0.1}(d) still has fluctuation.

\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-conv1d-epoch-20-}
        \caption{Conv1D}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-conv1d-epoch-40-}
        \caption{Conv1D}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-conv1d-epoch-60-}
        \caption{Conv1D}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-conv1d-epoch-60-LOSS-}
        \caption{Conv1D}
		\label{fig:c}
    \end{subfigure}
	\caption{In (a) deviation has nearly same characteristic with FNN in lower energies. After 60 epochs (c), there is a visible shift in higher energies but this does not occur in (b).}
\end{figure}

\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-E_int-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-E_pot-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-E_kin-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-0_1-epoch-60-E_Total-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{Separate Energy Predictions for $g = 0.1$ }
\label{fig:FFN-g-0.1-S}
\end{figure}

The order of interaction energy is much smaller than the others. This may be the reason of bias in (a). Applying a proper normalization to the variables may reduce the error. \textbf{REF}

\textbf{G = 1}

\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{FFN[128, 30, 30, 10, 1] results for $g = 1$}
\label{fig:FNN-g-1}
\end{figure}

Relative error gets smaller after each 20 epoch and network goes into saturation after 40 epoch.

\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-conv1d-epoch-20-}
        \caption{Conv1D}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-conv1d-epoch-40-}
        \caption{Conv1D}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-conv1d-epoch-60-}
        \caption{Conv1D}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-conv1d-epoch-60-LOSS-}
        \caption{Conv1D}
		\label{fig:c}
    \end{subfigure}
	\caption{CNN results for $g = 1$}
\label{fig:CNN-g-1}
\end{figure}

Even the loss of the CNN decreases slower than FNN Fig.~\ref{fig:FNN-g-1}(d) and involves local peeks, its relative error at epoch 40 and 60 is smaller. 

\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-E_int-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-E_pot-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-E_kin-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-1-epoch-60-E_Total-}
        \caption{FNN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{FNN[128, 30, 30, 10, 4], Separate energy predictions for $g = 1$.}
\label{fig:FFN-g-1-S}
\end{figure}

This is the worst case in our training results. In first sight, it can be thought that a proper normalization may reduce the error and increase the accuracy but the difference between energy values are not as higher as in the case $g = 0.1$, so the problem is not due to normalization. Here, contribution of interaction parameter to Eq.~\ref{eq:GPE_dimensionless} is in same order with other terms. So, it can be said that the degree of freedom of the network is not enough to handle such a situation.

\textbf{G=10}

\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{FNN[128, 30, 30, 10, 1] predictions for $g = 10$}
\label{fig:FFN-g-10}
\end{figure}

In Fig.~\ref{fig:FFN-g-10}(a) same deviation occurs in lower and higher energies as in previous results. Loss of the network decreases rapidly but order of it greater than the case of which $g = 1$


\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-conv1d-epoch-20-}
        \caption{Conv1D}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-conv1d-epoch-40-}
        \caption{Conv1D}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-conv1d-epoch-60-}
        \caption{Conv1D}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-conv1d-epoch-60-LOSS-}
        \caption{Conv1D}
		\label{fig:c}
    \end{subfigure}
	\caption{CNN results for $g = 10$}
\label{fig:CNN-g-10}
\end{figure}

Accuracy and precision of the CNN are greater than FFN's, both relative error and MSE are smaller than FNN predictions. Also unlike FFN, there is no bias in first 20 epochs. Another important subject is that last 20 epochs increases MSE but reduces relative error slightly. 

\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-E_int-}
        \caption{FFN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-E_pot-}
        \caption{FFN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-E_kin-}
        \caption{FFN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-10-epoch-60-E_Total-}
        \caption{FFN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{Separate prediction  g=10}
\label{fig:FFN-g-10-S}
\end{figure}

Result of separate energy prediction for $g=10$ is not poor as $g = 1$. This also shows that dominance of a term in the equation directly affects the prediction. 


\textbf{G = 20}

\svgpath{path = {"../figs/FFN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-20-}
        \caption{Epoch = 20}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-40-}
        \caption{Epoch = 40}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-}
        \caption{Epoch = 60}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-LOSS-}
        \caption{Loss}
		\label{fig:c}
    \end{subfigure}
	\caption{FFN[128, 30, 30, 10, 1] predictions for g = 20}
\label{fig:FFN-g-20}
\end{figure}

In Fig.~\ref{fig:FFN-g-20}(d), the network goes into saturation before 10 epochs. This shows that how the arrangement of the dataset and initialization of the network affects the predictions. A shuffling  on both dataset and network initialization may produce different loss graph. 

\svgpath{path = {"../figs/CNN/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-conv1d-epoch-20-}
        \caption{Conv1D}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-conv1d-epoch-40-}
        \caption{Conv1D}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-conv1d-epoch-60-}
        \caption{Conv1D}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-conv1d-epoch-60-LOSS-}
        \caption{Conv1D}
		\label{fig:d}
    \end{subfigure}
	\caption{CNN predictions for $g = 20$}
\label{fig:CNN-g-20}
\end{figure}

Unbiased predictions in first 20 epochs becomes biased after 40 epochs. It is because of energy distribution in the dataset. The number of examples at higher energies is greater than number of lowers. 

\svgpath{path = {"../figs/FFNMERGED/"}}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-E_int-}
        \caption{FFN[128, 30, 30, 10, 4]}
		\label{fig:a}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-E_pot-}
        \caption{FFN[128, 30, 30, 10, 4]}
		\label{fig:b}
    \end{subfigure}    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-E_kin-}
        \caption{FFN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=\linewidth]{potential-g-20-epoch-60-E_Total-}
        \caption{FFN[128, 30, 30, 10, 4]}
		\label{fig:c}
    \end{subfigure}
	\caption{Separate prediction  g=20}
\label{fig:FFN-g-20-S}
\end{figure}

The problem in Fig.~\ref{fig:FFN-g-20-S}(c) is same as in the case $g = 0.1$ which is shown in Fig.~\ref{fig:FFN-g-0.1-S}(a). The order of kinetic energy is  lower than other energies. 

\subsubsection{Inverse Problem Prediction of $\boldsymbol{g}$}


\svgpath{path = {"../figs/CNNgPrediction/"}}
\begin{figure}[H]
    \centering
    \includesvg[width=\linewidth]{fixed-pot_dens-g-VARY-conv1d-epoch-30-}
    \caption{Fixed potential and shift}
	\label{fig:a}
\end{figure}

\begin{figure}[H]
    \centering
    \includesvg[width=\linewidth]{various-pot_dens-g-VARY-conv1d-epoch-30-}
    \caption{Various potential and fixed shift}
	\label{fig:a}
\end{figure}

\begin{figure}[H]
    \centering
    \includesvg[width=\linewidth]{various-shift-pot_dens-g-VARY-conv1d-epoch-30-}
    \caption{Various potential and shift}
	\label{fig:a}
\end{figure}



\newpage

\end{document}