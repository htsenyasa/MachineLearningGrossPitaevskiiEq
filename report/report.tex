\documentclass[a4paper,times,12pt]{article}
\usepackage{amsmath}
%\usepackage{graphicx}
\usepackage{float}
\usepackage{setspace}
\usepackage{svg}
\usepackage{subcaption}
\usepackage{minted}

\onehalfspacing
\usepackage[top=2.5 cm, bottom=2.5 cm, left=4 cm, right=2.5 cm]{geometry}
\usepackage{hyperref}

\title{}%\textbf{Interaction Effects in Quantum Random Walk with atomic BEC}}

\date{}

\svgpath{path = {"../figs/archtestresults/"}}

\begin{document}


%\maketitle
\setcounter{page}{1}
\pagenumbering{roman}

\section*{Summary}

We train an artificial neural network to estimate the ground state energy
of a one-dimensional Bose-Einstein condensate in harmonic trapping potential.
Such a system can be described by the solution of a corresponding
Gross-Pitaevskii equation also called a non-linear Schroedinger equation.
We also use the method to predict the non-linearity parameter using the ground
state density profile for a given harmonic trapping potential.


\section{Introduction}
\section{Gross Pitaevskii Equation}
\subsection{Types of Potentials}
\subsection{Analytical Solution and Approximation}
\subsection{Numerical Solution and XMDS Framework}
\section{Problem Statements and Dataset Generation}

GPE can be solved with briefly described methods above. These methods must be applied every time when there is a change in the equation such as different trapping potential or parameters. For example, when interaction parameter is changed, if there is no analytic solution, numerical method must be reapplied. With machine learning these steps can be bypassed and information about the system can be obtained instantly with only one time cost which is training process of the network. \textbf{REF}. Here \textbf{REF}, authors managed to apply deep learning method to the Schrodinger Equation to obtain ground state energy of the system under different potentials. 

We try to built a neural network to predict ground state energy of a BEC for a given harmonic trapping potential and interaction parameter. We also try inverse problem which is prediction of interaction parameter for a given potential and density function. 


An artificial neural network is made of layers and these layers contains simple units called neurons. These neurons can be thought as primitive version of the neurons in the human brain. A neuron takes one or more input and generates an output. To do this, it uses its internal variables which are called weight and bias. In general, number of weights in a neuron is equal to the input size of the neuron. Input output relation of a neuron can be described with the following way. 

Let $\boldsymbol{x} = [x_1, x_2, x_3, ..., x_n]$ be input of the neuron and $f$ be the function that describes behavior of the neuron. 

\begin{equation}
\label{eq:NU_neuron}
f(\boldsymbol{x}, \boldsymbol{\omega}, b) = \sum\limits_{i = 1}^n x_i\omega_i + b
\end{equation}

Here, $\omega_i$ is weight and $b$ is bias. A layer of a network involves neurons behaves like this function. In our example, $x_n$ inputs represent the potential expression respect to the position or density function. \textbf{FIGURE OF NETWORK and explanation}



\begin{equation}
\label{eq:NU_neuron_nonlinear}
f_2(f_1(\boldsymbol{x}, \boldsymbol{\omega}, b)) = \sum\limits_{i=1}^{n_2} \sum\limits_{j=1}^{n_1} (x_j\omega_j + b_1)\omega_i + b_2
\end{equation}



Connection complexity of the neurons may vary. Output of each neuron in one layer can be input of a neuron in next layer. Such a networks are called Fully Connected Networks. \textbf{REF}. If result of one layer directly is sent to the next layer without any circulation or feedback, then the network is called as Feed Forward Network. \textbf{REF}

We are going to change these weights and biases in such a way that the difference between output generated by network and real value of the corresponding quantity will be minimized. To do that, we are going to use a proxy relation between output and real value which is called as Cost Function. \textbf{REF}. There are many cost functions such as quadratic cost function, cross entropy cost function etc. We are going to use quadratic cost function also called as mean squared error to show general mechanism and to introduce few notions that directly effects the behavior of the network. 

Quadratic cost function is defined as;

\begin{equation}
\label{eq:NT_Quadratic}
C(\omega, b) = \frac{1}{2n} \sum\limits_{x} || \boldsymbol{a} - f_L(\boldsymbol{x}) ||{^2} 
\end{equation}

where $\omega$, $b$ are weights and biases respectively. $f_L(\boldsymbol{x})$ output produced by network and $\boldsymbol{a}$ is real value of the quantity. To minimize C, we can take the derivative of the function and can find extremum values, but this method is extremely costly because the total number of neurons in the network is enormous. \textbf{REF}. This problem is overcame by an iterative algorithm called Gradient Descent. Since $C$ is also a scalar field the algorithm tries to determine a direction to which points the decrement and moves to that direction with small steps. In each iteration, algorithm repeats itself to reach minimum value. 

A small displacement in arbitrary direction which corresponds to small change in weight or bias or both can be written as;
\begin{equation}
\label{eq:NT_Quadratic_min}
\Delta{C} = \frac{\partial{C}}{\partial{\omega}}\Delta{\omega} + \frac{\partial{C}}{\partial{b}}\Delta{b}
\end{equation}

This expression gives information about what happens when small displacement is done but it does not give any information about direction of decrement. To determine direction of decrement we can rewrite this expression in terms of gradient since it is by definition points to direction of maximum rate of increase. 

We can define gradient operator as;

\begin{equation}
\label{eq:gradient}
\boldsymbol{\nabla}{C} = \left(\frac{\partial{C}}{\partial{\omega}},\frac{\partial{C}}{\partial{b}} \right)^T
\end{equation}

In this case, Eq.~\ref{NT_Quadratic_min} can be written as;

\begin{equation}
\label{eq:NT_Quadratic_min_gradient_form}
\Delta{C} = \boldsymbol{\nabla}{C} \boldsymbol{.} \Delta \boldsymbol{l}
\end{equation}

where $\boldsymbol{l} = (\omega, b)^T$. What we want is again minimize cost function, thus; in each iteration value of the C must be smaller than previous one which is equally saying that $\Delta{C}$ must be smaller than zero, therefore; direction of displacement must be in the opposite direction of gradient. Rather than calculating the opposite direction on each iteration we can define $\Delta{\boldsymbol{l}}$ in such a way so that it always points to opposite direction of the gradient.

\begin{equation}
\label{eq:NT_learning_rate}
\Delta{\boldsymbol{l}} = -\eta\boldsymbol{\nabla}C
\end{equation}

In this case, Eq.~\ref{eq:NT_Quadratic_min_gradient_form} becomes;

\begin{equation}
\label{eq:NT_Displacement}
\Delta{C} = -\eta ||\boldsymbol{\nabla{C}}||^2
\end{equation}

where $\eta$ is positive definite number and it is called as learning rate.

\subsection{Dataset and Dataset Generation}

As mentioned, we solved NLSE for four different interaction parameters, therefore; there are four different corresponding datasets and each of them contains 10.000 elements. We used 8500 of them as training examples and 1500 of them for test. An element of a dataset involves an array containing potential values respect to the position and an another array containing interaction, kinetic, potential and total energy values respectively. 

The array containing potential energy is generated according to harmonic trap expression in XMDS, thus; \textbf{randomness} of the dataset occurs in angular frequency and shift of equilibrium point. We supplied these random angular frequency and shift values to the XMDS within predetermined limits. Angular frequency can take values between 0.5 and 2. Shift of the equilibrium point is determined due to boundary conditions since \textbf{density function} must be zero at boundaries. In numerical solution of the NLSE, domain of the \textbf{transverse dimension} is between -10 to 10. Taking the maximum magnitude of the shift as $\mp 5$ enough to ensure that density function goes to zero at infinity.


\section{Machine Learning for NLSE}

We used Pytorch Framework to build our neural networks. It allows the client codes work on both CPU and GPU via its internal python object called Tensor. If any CUDA supported GPU is available then Pytorch can use GPU without any change in the code. Code have three main parts, first one is dataloaders; it reads train and test data for specified interaction parameter from corresponding file and generates tensor dataset object. \textbf{continue}

We implemented two different types of neural network; feed forward (FNN) and convolutional neural network (CNN).


\subsection{Architecture}

FNN involves 128 input neurons as input layer, next layer is first hidden layer with 30 neurons, the second is same as first hidden layer, the next one is last hidden layer with 10 neurons and the last layer is output layer. Totally there are 5 layers in our FFN and it will be denoted as $FNN[128, 30, 30, 10, 4]$. Rectified linear unit (ReLU) is used for each forward except output. No operation is applied to the output neuron. Learning rate of the FNN is fixed and it is $0.001$. Cost function is mean squared error (MSE) and optimization is done with Adam. 


CNN has two convolution layers, two maxpool layers and three fully connected layer and last layer of the fully connected part is output layer. Maxpooling is applied to output of the first and second convolution layers. ReLu is also applied each forward except output neuron same as in the FNN. Fully connected part of the CNN is $FNN[310, 100, 20, 1]$. Learning rate, cost function and optimizer of the CNN is same as FNN which are $0.001$, MSE and Adam respectively.


\subsection{Hyperparameters}

We firstly started with smaller dataset which involves 800 elements for trainig 200 for test to see response of the architecture and to obtain a range for learning rate.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
           \centering
            \includesvg[width=8cm]{{"fourlayer-g0-lr0_003-800"}}
            \caption{FNN[128, 40, 40, 1], lr = 0.003}
            \label{fig:a}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
             \includesvg[width=8cm]{{"fourlayer-g0-lr0_001-800"}}
            \caption{FNN[128, 40, 40, 1], lr = 0.001}
            \label{fig:b}
    \end{subfigure}
    \caption{In this example interaction parameter g, is set to zero. Even learning rate 0.001 seems more accurate, it is trivial to expect that change in interaction parameter will effect the result. }
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
           \centering
            \includesvg[width=8cm]{{"fourlayer-g10-lr0_003-800"}}
            \caption{FNN[128, 40, 40, 1], lr = 0.003}
            \label{fig:a}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
             \includesvg[width=8cm]{{"fourlayer-g10-lr0_001-800"}}
            \caption{FNN[128, 40, 40, 1], lr = 0.001}
            \label{fig:b}
    \end{subfigure}
    \caption{Here, interaction parameter, g is 10. Predictions of the network with learning rate 0.003 more accurate than 0.001. }
\end{figure}

\textbf{comment about range of the learning rate ?}

First architecture we used had only 4 layers and it was not sufficient.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
           \centering
            \includesvg[width=8cm]{fivelayer-g10-800}
            \caption{FNN[128, 30, 30, 10, 1]}
            \label{fig:a}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.45\textwidth}
            \centering
             \includesvg[width=8cm]{fourlayer-g10-800}
            \caption{FNN[128, 40, 40, 1]}
            \label{fig:b}
    \end{subfigure}
    \caption{Here interaction paramater g, is again set to 10. Batch size is 10 and total number of epoch is 20 for this dataset. }
\end{figure}


Then we increased the number of layers to 5 and tried different number of neuron combinations. Each of these tried network behaved different for different g values. For example, MSE of the FNN[128, 40, 40, 20, 1] for g = 10 is $5.11 x 10^{-3}$ but for g = 0 it is $2.00 x 10^{-3}$


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=8cm]{fivelayer-g10-3500}
        \caption{FNN[128, 30, 30, 10, 1]}
		\label{fig:a}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=8cm]{fourlayer-g10-3500}
        \caption{FNN[128, 40, 40, 1]}
		\label{fig:b}
    \end{subfigure}
    \caption{3500/500}
\end{figure}


After narrowing the number of candidate architectures with small dataset, we increased the size of the dataset to 3500/500 to get more information about distribution of the predictions and to determine other two hyperparameters; mini batch size and epoch. \textbf{Add results of this arch}.

\svgpath{path = {"../figs/FFN/"}}


\subsection{Training Results}
\subsubsection{Non-interacting System}

In non-interacting system, GPE reduces to SE which does not involve non-linear term. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=8cm]{potential-g-0-epoch-20-}
        \caption{FNN[128, 30, 30, 10, 1]}
		\label{fig:a}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.45\textwidth}
		\centering
        \includesvg[width=8cm]{potential-g-0-epoch-40-}
        \caption{FNN[128, 30, 30, 10, 1]}
		\label{fig:a}
    \end{subfigure}\hfill    
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includesvg[width=8cm]{potential-g-0-epoch-60-}
        \caption{FNN[128, 40, 40, 1]}
		\label{fig:b}
    \end{subfigure}
    \caption{Since the system has no interaction parameter, problem is relatively easy when it is compared to systems that involve interaction. In (a) it is obvious network requires more training example and it reaches a satisfactory level in (b). Supplying 20 more epochs nearly does not effect the prediction accuracy.  }

asdasd
asda

\subsubsection{Interacting Systems}



    

\end{figure}


\newpage

\end{document}