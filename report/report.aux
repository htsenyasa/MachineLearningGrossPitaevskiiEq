\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{i}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Gross Pitaevskii Equation}{i}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Types of Potentials}{i}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Analytical Solution and Approximation}{i}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Numerical Solution and XMDS Framework}{i}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Statements and Dataset Generation}{i}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset and Dataset Generation}{ii}{subsection.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Machine Learning for NLSE}{ii}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Architecture}{ii}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Hyperparameters}{iii}{subsection.4.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:a}{{1a}{iii}{FNN[128, 40, 40, 1], lr = 0.003\relax }{figure.caption.2}{}}
\newlabel{sub@fig:a}{{a}{iii}{FNN[128, 40, 40, 1], lr = 0.003\relax }{figure.caption.2}{}}
\newlabel{fig:b}{{1b}{iii}{FNN[128, 40, 40, 1], lr = 0.001\relax }{figure.caption.2}{}}
\newlabel{sub@fig:b}{{b}{iii}{FNN[128, 40, 40, 1], lr = 0.001\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces In this example interaction parameter g, is set to zero. Even learning rate 0.001 seems more accurate, it is trivial to expect that change in interaction parameter will effect the result. \relax }}{iii}{figure.caption.2}}
\newlabel{fig:a}{{2a}{iii}{FNN[128, 40, 40, 1], lr = 0.003\relax }{figure.caption.3}{}}
\newlabel{sub@fig:a}{{a}{iii}{FNN[128, 40, 40, 1], lr = 0.003\relax }{figure.caption.3}{}}
\newlabel{fig:b}{{2b}{iii}{FNN[128, 40, 40, 1], lr = 0.001\relax }{figure.caption.3}{}}
\newlabel{sub@fig:b}{{b}{iii}{FNN[128, 40, 40, 1], lr = 0.001\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Here, interaction parameter, g is 10. Predictions of the network with learning rate 0.003 more accurate than 0.001. \relax }}{iii}{figure.caption.3}}
\newlabel{fig:a}{{3a}{iv}{FNN[128, 30, 30, 10, 1]\relax }{figure.caption.4}{}}
\newlabel{sub@fig:a}{{a}{iv}{FNN[128, 30, 30, 10, 1]\relax }{figure.caption.4}{}}
\newlabel{fig:b}{{3b}{iv}{FNN[128, 40, 40, 1]\relax }{figure.caption.4}{}}
\newlabel{sub@fig:b}{{b}{iv}{FNN[128, 40, 40, 1]\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Here interaction paramater g, is again set to 10. Batch size is 10 and total number of epoch is 20 for this dataset. \relax }}{iv}{figure.caption.4}}
\newlabel{fig:a}{{4a}{iv}{FNN[128, 30, 30, 10, 1]\relax }{figure.caption.5}{}}
\newlabel{sub@fig:a}{{a}{iv}{FNN[128, 30, 30, 10, 1]\relax }{figure.caption.5}{}}
\newlabel{fig:b}{{4b}{iv}{FNN[128, 40, 40, 1]\relax }{figure.caption.5}{}}
\newlabel{sub@fig:b}{{b}{iv}{FNN[128, 40, 40, 1]\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces 3500/500\relax }}{iv}{figure.caption.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Training Results}{v}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Non-interacting System}{v}{subsubsection.4.3.1}}
\newlabel{fig:a}{{5a}{v}{FNN[128, 30, 30, 10, 1]\relax }{figure.caption.6}{}}
\newlabel{sub@fig:a}{{a}{v}{FNN[128, 30, 30, 10, 1]\relax }{figure.caption.6}{}}
\newlabel{fig:a}{{5b}{v}{FNN[128, 30, 30, 10, 1]\relax }{figure.caption.6}{}}
\newlabel{sub@fig:a}{{b}{v}{FNN[128, 30, 30, 10, 1]\relax }{figure.caption.6}{}}
\newlabel{fig:b}{{5c}{v}{FNN[128, 40, 40, 1]\relax }{figure.caption.6}{}}
\newlabel{sub@fig:b}{{c}{v}{FNN[128, 40, 40, 1]\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Since the system has no interaction parameter, problem is relatively easy when it is compared to systems that involve interaction. In (a) it is obvious network requires more training example and it reaches a satisfactory level in (b). Supplying 20 more epochs nearly does not effect the prediction accuracy. \relax }}{v}{figure.caption.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Interacting Systems}{v}{subsubsection.4.3.2}}
